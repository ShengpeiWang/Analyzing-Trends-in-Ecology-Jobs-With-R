knitr::opts_chunk$set(echo = TRUE)
# An example of using this function.
position_breakdown(page_4i18)
library('rvest')    # Parsing of HTML/XML files
library(tidyverse)  # General-purpose data wrangling
library(stringr)    # String manipulation
library(ggplot2)
library(tidytext)       # provides additional text mining functions
library(rebus)      # Verbose regular expressions
library(lubridate)  # Eases DateTime manipulation
page_archive <- read_html("https://listserv.umd.edu/archives/ecolog-l.html")
# I noticed that by interacting with html source code of the full list, all the weekly links are stored under the "li" node
pages_info  <- page_archive %>%  html_nodes("li")  # Scrap the weekly link info from main page
urls <- lapply(pages_info, function(x) str_sub(x,15, 52)) # get only the url part, which is between 15-52 characters.
urls <- urls[-c(1:4)] # remove the first 4 items that are not related to our purpose.
# Making the list of urls
list_of_pages <- str_c('https://listserv.umd.edu', urls)
head(list_of_pages)
#I need to create a custom function that gets all postings from each page
ecolog_p <- function(url){
webpage <- read_html(url)
Sys.sleep(0.2)
webpage %>% html_nodes("ul:nth-child(3) a") %>%
html_text
}
# Here is an example of how this works. I'm not scrapping the entire list here, because it takes about two hours to run with good internet connections.
# ecolog_postings_1_25 <- lapply(list_of_pages[1:25], ecolog_p)
url <- "https://listserv.umd.edu/cgi-bin/wa?A1=ind1804a&L=ecolog-l"
webpage <- read_html(url)
page_4i18 <- html_text(html_nodes(webpage,"ul:nth-child(3) a")) #First download the webpage
#I wrote another function to count the apearance of different key words.
position_breakdown <- function(x){
x = str_to_lower(x)
u = sum(str_count(x, "reu|intern|volunteer"))
m = sum(str_count(x, "ms |m.s |msc |m.sc "))
phd = sum(str_count(x, "phd"))
po = sum(str_count(x, "postdoc|post-doc|post doc"))
f = sum(str_count(x, "director|faculty|professor|instructor"))
o = sum(str_count(x, "technician|assistant|manager"))
l = length(x)
y = data.frame(undergrad = u, ms = m, phd = phd, postdoc = po,
faculty = f, otherjob = o, l = l)
y}
# An example of using this function.
position_breakdown(page_4i18)
page_4i18
webpage
html_nodes(webpage)
html_node(webpage)
page_4i18 <- html_text(html_nodes(webpage,"a"))
page_4i18
page_4i18 <- html_text(html_nodes(webpage,"tr:nth-child(3) a"))
page_4i18
url <- "https://listserv.umd.edu/cgi-bin/wa?A1=ind1804a&L=ecolog-l"
webpage <- read_html(url)
page_4i18 <- html_text(html_nodes(webpage,"tr:nth-child(3) a")) #First download the webpage
# A quick note that I used the web tool "Selector Gadget"" to generate the CSS selectors. i.e. the ""tr:nth-child(3) a"" part
#I wrote another function to count the apearance of different key words.
position_breakdown <- function(x){
x = str_to_lower(x)
u = sum(str_count(x, "reu|intern|volunteer"))
m = sum(str_count(x, "ms |m.s |msc |m.sc "))
phd = sum(str_count(x, "phd"))
po = sum(str_count(x, "postdoc|post-doc|post doc"))
f = sum(str_count(x, "director|faculty|professor|instructor"))
o = sum(str_count(x, "technician|assistant|manager"))
l = length(x)
y = data.frame(undergrad = u, ms = m, phd = phd, postdoc = po,
faculty = f, otherjob = o, l = l)
y}
# An example of using this function.
position_breakdown(page_4i18)
page_4i18[1]
#I wrote another function to count the apearance of different key words.
position_breakdown <- function(x){
x = str_to_lower(x)
u = sum(str_count(x, "reu|intern|volunteer"))
m = sum(str_count(x, "ms |m.s |msc |m.sc "))
phd = sum(str_count(x, "phd"))
po = sum(str_count(x, "postdoc|post-doc|post doc"))
f = sum(str_count(x, "director|faculty|professor|instructor"))
o = sum(str_count(x, "technician|assistant|manager"))
l = length(x)
y = data.frame(undergrad = u, ms = m, phd = phd, postdoc = po,
faculty = f, otherjob = o, total = l)
y}
# An example of using this function.
position_breakdown(page_4i18)
library('rvest')    # Parsing of HTML/XML files
library(tidyverse)  # General-purpose data wrangling
library(stringr)    # String manipulation
library(ggplot2)
library(tidytext)       # provides additional text mining functions
library(rebus)      # Verbose regular expressions
library(lubridate)  # Eases DateTime manipulation
